{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep500 Recipe Tutorial\n",
    "A recipe is composed of: fixed components (that cannot change if benchmarked), mutable components (that can be tuned), and acceptable metrics.\n",
    "\n",
    "First thing's first, let's import Deep500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: mpi4py not available, distributed optimization disabled\n"
     ]
    }
   ],
   "source": [
    "import deep500 as d5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set up the fixed components of the recipe. In this example, we will use a LeNet CNN on the MNIST dataset, both of which are available in `deep500.networks` and `deep500.datasets`. We will also set an epoch budget of 10. More information may be found by browsing the available [datasets](https://github.com/deep500/deep500/tree/master/deep500/datasets) and [networks](https://github.com/deep500/deep500/tree/master/deep500/networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_components = {\n",
    "    'dataset': 'mnist',\n",
    "    'train_sampler': d5.ShuffleSampler,\n",
    "    'model': 'simple_cnn',\n",
    "    'epochs': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our benchmark may want to allow different executors (i.e., deep learning frameworks such as TensorFlow), batch sizes and optimizers. We will define them as mutable components. Before we do that, we must import an executor, for instance PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: mpi4py not available, distributed optimization disabled\n"
     ]
    }
   ],
   "source": [
    "from deep500.frameworks import pytorch as d5fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable_components = {\n",
    "    'executor': d5fw.from_model,\n",
    "    'executor_kwargs': dict(device=d5.GPUDevice()),  # Let the executor know that the default device is GPU\n",
    "    'batch_size': 64,\n",
    "    'optimizer': d5fw.MomentumOptimizer,  # Use a framework built-in optimizer for performance\n",
    "    'optimizer_args': (0.1, 0.9) # Set learning rate to 0.1, momentum to 0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all that's left is to define what are our success criteria. Since MNIST is relatively easy to train, we will set 90% validation accuracy as our target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_metrics = [\n",
    "    (d5.TestAccuracy(), 90)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\deep500\\networks\\pytorch_mnist.py:11: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(m.weight, gain=np.sqrt(2))\n",
      "C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\deep500\\networks\\pytorch_mnist.py:12: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(m.bias, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input.1 : Float(64, 1, 28, 28),\n",
      "      %conv1.weight : Float(10, 1, 5, 5),\n",
      "      %conv1.bias : Float(10),\n",
      "      %conv2.weight : Float(20, 10, 5, 5),\n",
      "      %conv2.bias : Float(20),\n",
      "      %fc1.weight : Float(50, 320),\n",
      "      %fc1.bias : Float(50),\n",
      "      %fc2.weight : Float(10, 50),\n",
      "      %fc2.bias : Float(10)):\n",
      "  %9 : Float(64, 10, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%input.1, %conv1.weight, %conv1.bias), scope: Net/Conv2d[conv1] # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %10 : Float(64, 10, 12, 12) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%9), scope: Net # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %11 : Float(64, 10, 12, 12) = onnx::Relu(%10), scope: Net # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\functional.py:913:0\n",
      "  %12 : Float(64, 20, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%11, %conv2.weight, %conv2.bias), scope: Net/Conv2d[conv2] # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %13 : Float(64, 20, 4, 4) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%12), scope: Net # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %14 : Float(64, 20, 4, 4) = onnx::Relu(%13), scope: Net # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\functional.py:913:0\n",
      "  %15 : Tensor = onnx::Constant[value=  -1  320 [ Variable[CPULongType]{2} ]](), scope: Net\n",
      "  %16 : Float(64, 320) = onnx::Reshape(%14, %15), scope: Net # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\deep500\\networks\\pytorch_mnist.py:33:0\n",
      "  %17 : Float(64, 50) = onnx::Gemm[alpha=1, beta=1, transB=1](%16, %fc1.weight, %fc1.bias), scope: Net/Linear[fc1] # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  %18 : Float(64, 50) = onnx::Relu(%17), scope: Net # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\functional.py:913:0\n",
      "  %19 : Float(64, 10) = onnx::Gemm[alpha=1, beta=1, transB=1](%18, %fc2.weight, %fc2.bias), scope: Net/Linear[fc2] # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  return (%19)\n",
      "\n",
      "Download complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████| 156/156 [00:02<00:00, 69.71it/s, accuracy=5.02, test_loss=2.3]\n",
      "Training (epoch 1/10): 100%|████████████████████████| 937/937 [00:07<00:00, 119.41it/s, batch_acc=92.2, loss_avg=0.222]\n",
      "Testing: 100%|██████████████████████████████████████| 156/156 [00:00<00:00, 239.26it/s, accuracy=96.9, test_loss=0.114]\n",
      "Training (epoch 2/10): 100%|████████████████████████| 937/937 [00:07<00:00, 118.67it/s, batch_acc=100, loss_avg=0.0981]\n",
      "Testing: 100%|██████████████████████████████████████| 156/156 [00:00<00:00, 238.17it/s, accuracy=97.8, test_loss=0.079]\n",
      "Training (epoch 3/10): 100%|█████████████████████████| 937/937 [00:07<00:00, 118.49it/s, batch_acc=96.9, loss_avg=0.08]\n",
      "Testing: 100%|██████████████████████████████████████| 156/156 [00:00<00:00, 235.29it/s, accuracy=98.1, test_loss=0.069]\n",
      "Training (epoch 4/10): 100%|███████████████████████| 937/937 [00:07<00:00, 118.25it/s, batch_acc=98.4, loss_avg=0.0838]\n",
      "Testing: 100%|█████████████████████████████████████| 156/156 [00:00<00:00, 234.59it/s, accuracy=98.3, test_loss=0.0644]\n",
      "Training (epoch 5/10): 100%|███████████████████████| 937/937 [00:07<00:00, 117.54it/s, batch_acc=98.4, loss_avg=0.0786]\n",
      "Testing: 100%|█████████████████████████████████████| 156/156 [00:00<00:00, 240.37it/s, accuracy=97.7, test_loss=0.0882]\n",
      "Training (epoch 6/10): 100%|███████████████████████| 937/937 [00:07<00:00, 117.82it/s, batch_acc=98.4, loss_avg=0.0709]\n",
      "Testing: 100%|███████████████████████████████████████| 156/156 [00:00<00:00, 235.65it/s, accuracy=98, test_loss=0.0783]\n",
      "Training (epoch 7/10): 100%|███████████████████████| 937/937 [00:07<00:00, 117.58it/s, batch_acc=98.4, loss_avg=0.0704]\n",
      "Testing: 100%|█████████████████████████████████████| 156/156 [00:00<00:00, 235.30it/s, accuracy=98.2, test_loss=0.0812]\n",
      "Training (epoch 8/10): 100%|███████████████████████| 937/937 [00:07<00:00, 117.49it/s, batch_acc=96.9, loss_avg=0.0681]\n",
      "Testing: 100%|█████████████████████████████████████| 156/156 [00:00<00:00, 234.23it/s, accuracy=98.1, test_loss=0.0766]\n",
      "Training (epoch 9/10): 100%|███████████████████████| 937/937 [00:07<00:00, 117.80it/s, batch_acc=96.9, loss_avg=0.0898]\n",
      "Testing: 100%|█████████████████████████████████████| 156/156 [00:00<00:00, 233.18it/s, accuracy=98.1, test_loss=0.0779]\n",
      "Training (epoch 10/10): 100%|██████████████████████| 937/937 [00:08<00:00, 116.33it/s, batch_acc=96.9, loss_avg=0.0978]\n",
      "Testing: 100%|██████████████████████████████████████| 156/156 [00:00<00:00, 232.49it/s, accuracy=96.5, test_loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestAccuracy: 96.49439102564104\n",
      "WallclockTime: 88.43 seconds\n",
      "PASSED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d5.run_recipe(fixed_components, mutable_components, acceptable_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset is not there, Deep500 will automatically download it for you (or provide instructions to set it up). As we can see, the test accuracy goes up from 5.02% before training to 92.2% on the first epoch, maxing out at around 98%. We can improve this by changing the neural network, or by adding regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \"1e-4 * w\" as a term to the gradient\n",
    "def weight_decay(name: str, param, grad):\n",
    "    grad += 1e-4 * param\n",
    "    return grad\n",
    "\n",
    "# Set the optimizer to modify the gradients to include this\n",
    "mutable_components['optimizer_kwargs'] = dict(gradient_modifier=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input.1 : Float(64, 1, 28, 28),\n",
      "      %conv1.weight : Float(10, 1, 5, 5),\n",
      "      %conv1.bias : Float(10),\n",
      "      %conv2.weight : Float(20, 10, 5, 5),\n",
      "      %conv2.bias : Float(20),\n",
      "      %fc1.weight : Float(50, 320),\n",
      "      %fc1.bias : Float(50),\n",
      "      %fc2.weight : Float(10, 50),\n",
      "      %fc2.bias : Float(10)):\n",
      "  %9 : Float(64, 10, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%input.1, %conv1.weight, %conv1.bias), scope: Net/Conv2d[conv1] # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %10 : Float(64, 10, 12, 12) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%9), scope: Net # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %11 : Float(64, 10, 12, 12) = onnx::Relu(%10), scope: Net # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\functional.py:913:0\n",
      "  %12 : Float(64, 20, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%11, %conv2.weight, %conv2.bias), scope: Net/Conv2d[conv2] # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %13 : Float(64, 20, 4, 4) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%12), scope: Net # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %14 : Float(64, 20, 4, 4) = onnx::Relu(%13), scope: Net # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\functional.py:913:0\n",
      "  %15 : Tensor = onnx::Constant[value=  -1  320 [ Variable[CPULongType]{2} ]](), scope: Net\n",
      "  %16 : Float(64, 320) = onnx::Reshape(%14, %15), scope: Net # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\deep500\\networks\\pytorch_mnist.py:33:0\n",
      "  %17 : Float(64, 50) = onnx::Gemm[alpha=1, beta=1, transB=1](%16, %fc1.weight, %fc1.bias), scope: Net/Linear[fc1] # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  %18 : Float(64, 50) = onnx::Relu(%17), scope: Net # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\functional.py:913:0\n",
      "  %19 : Float(64, 10) = onnx::Gemm[alpha=1, beta=1, transB=1](%18, %fc2.weight, %fc2.bias), scope: Net/Linear[fc2] # C:\\Users\\XL\\miniconda3\\envs\\denv\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  return (%19)\n",
      "\n",
      "Download complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████| 156/156 [00:00<00:00, 233.54it/s, accuracy=7.9, test_loss=2.29]\n",
      "Training (epoch 1/10): 100%|████████████████████████| 937/937 [00:09<00:00, 103.62it/s, batch_acc=98.4, loss_avg=0.201]\n",
      "Testing: 100%|█████████████████████████████████████| 156/156 [00:00<00:00, 222.86it/s, accuracy=97.5, test_loss=0.0967]\n",
      "Training (epoch 2/10): 100%|███████████████████████| 937/937 [00:09<00:00, 101.09it/s, batch_acc=98.4, loss_avg=0.0952]\n",
      "Testing: 100%|█████████████████████████████████████| 156/156 [00:00<00:00, 242.61it/s, accuracy=98.2, test_loss=0.0623]\n",
      "Training (epoch 3/10): 100%|████████████████████████| 937/937 [00:08<00:00, 104.16it/s, batch_acc=100, loss_avg=0.0828]\n",
      "Testing: 100%|██████████████████████████████████████| 156/156 [00:00<00:00, 228.08it/s, accuracy=96.7, test_loss=0.116]\n",
      "Training (epoch 4/10): 100%|█████████████████████████| 937/937 [00:09<00:00, 96.44it/s, batch_acc=100, loss_avg=0.0755]\n",
      "Testing: 100%|██████████████████████████████████████| 156/156 [00:00<00:00, 201.55it/s, accuracy=96.8, test_loss=0.139]\n",
      "Training (epoch 5/10): 100%|█████████████████████████| 937/937 [00:09<00:00, 95.89it/s, batch_acc=100, loss_avg=0.0665]\n",
      "Testing: 100%|█████████████████████████████████████| 156/156 [00:00<00:00, 220.96it/s, accuracy=98.4, test_loss=0.0559]\n",
      "Training (epoch 6/10): 100%|████████████████████████| 937/937 [00:11<00:00, 83.24it/s, batch_acc=98.4, loss_avg=0.0581]\n",
      "Testing: 100%|█████████████████████████████████████| 156/156 [00:01<00:00, 133.56it/s, accuracy=98.4, test_loss=0.0557]\n",
      "Training (epoch 7/10): 100%|█████████████████████████| 937/937 [00:13<00:00, 69.55it/s, batch_acc=100, loss_avg=0.0549]\n",
      "Testing: 100%|█████████████████████████████████████| 156/156 [00:00<00:00, 172.00it/s, accuracy=98.3, test_loss=0.0556]\n",
      "Training (epoch 8/10): 100%|████████████████████████| 937/937 [00:11<00:00, 83.41it/s, batch_acc=96.9, loss_avg=0.0505]\n",
      "Testing: 100%|█████████████████████████████████████| 156/156 [00:00<00:00, 195.00it/s, accuracy=97.6, test_loss=0.0893]\n",
      "Training (epoch 9/10): 100%|████████████████████████| 937/937 [00:13<00:00, 71.52it/s, batch_acc=96.9, loss_avg=0.0462]\n",
      "Testing: 100%|█████████████████████████████████████| 156/156 [00:01<00:00, 123.22it/s, accuracy=98.3, test_loss=0.0692]\n",
      "Training (epoch 10/10): 100%|███████████████████████| 937/937 [00:16<00:00, 56.08it/s, batch_acc=98.4, loss_avg=0.0565]\n",
      "Testing: 100%|█████████████████████████████████████| 156/156 [00:01<00:00, 143.65it/s, accuracy=98.6, test_loss=0.0516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestAccuracy: 98.61778846153847\n",
      "WallclockTime: 105.28 seconds\n",
      "WallclockTime: 122.13 seconds\n",
      "PASSED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d5.run_recipe(fixed_components, mutable_components, acceptable_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the accuracy is 98.6%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
