{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep500 without recipes\n",
    "\n",
    "This tutorial shows how to construct and run a benchmark manually, without the recipe interface.\n",
    "We will train CIFAR-10 for 5 epochs.\n",
    "\n",
    "First, import the packages for Deep500, datasets, and networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: mpi4py not available, distributed optimization disabled\n"
     ]
    }
   ],
   "source": [
    "import deep500 as d5\n",
    "from deep500 import datasets as d5ds\n",
    "from deep500 import networks as d5nt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We now need to create a DNN (ResNet-20), which requires the dimensions of the dataset samples. The first step is to obtain the sample shape and loss function information from CIFAR-10, as well as set a minibatch size (due to ONNX static sizes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 32, 32) <class 'deep500.utils.onnx_interop.losses.SoftmaxCrossEntropy'>\n"
     ]
    }
   ],
   "source": [
    "shape = d5ds.dataset_shape('cifar10')\n",
    "loss = d5ds.dataset_loss('cifar10')\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "print(shape, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of `dataset_shape` is organized as (number of classes, sample dimensions...).\n",
    "\n",
    "The next part is to construct the model. In `deep500.networks`, we provide a built-in generator for ResNet architectures (requires PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 21 superfluous nodes in graph\n"
     ]
    }
   ],
   "source": [
    "model, input_node, output_node = d5nt.create_model('resnet', depth=20, classes=shape[0], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the loss to the end of the model, in order to be able to train the classification problem (the names 'label' and 'loss' are arbitrary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add_operation(loss([output_node, 'label'], 'loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Constructing the dataset requires knowing the input and target nodes, so we construct it after the model. We construct both training and validation set at the same time. For training, we also create a sampler that randomizes access by shuffling the data every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete.\n",
      "\n",
      "unzipping in path: C:\\Users\\XL\\AppData\\Local\\Temp\\cifar10\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "train_set, validation_set = d5ds.load_dataset('cifar10', input_node, 'label')\n",
    "train_sampler = d5.ShuffleSampler(train_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executor\n",
    "\n",
    "We use PyTorch as our graph executor, constructing it from the model, and set the GPU to be the main device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: mpi4py not available, distributed optimization disabled\n"
     ]
    }
   ],
   "source": [
    "from deep500.frameworks import pytorch as d5fw\n",
    "executor = d5fw.from_model(model, device=d5.GPUDevice())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "We choose the framework-native momentum optimizer, and construct it with our executor and loss node name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = d5fw.MomentumOptimizer(executor, learning_rate=0.1, momentum=0.9, loss='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Everything is ready, now we can train. We add some global metrics to show how to customize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████| 156/156 [00:03<00:00, 44.03it/s, accuracy=10, test_loss=2.29]\n",
      "Training (epoch 1/5): 100%|███████████████████████████| 781/781 [00:43<00:00, 18.08it/s, batch_acc=62.5, loss_avg=1.49]\n",
      "Testing: 100%|████████████████████████████████████████| 156/156 [00:02<00:00, 59.45it/s, accuracy=50.1, test_loss=1.44]\n",
      "Training (epoch 2/5): 100%|██████████████████████████| 781/781 [00:46<00:00, 16.82it/s, batch_acc=57.8, loss_avg=0.992]\n",
      "Testing: 100%|███████████████████████████████████████| 156/156 [00:02<00:00, 52.28it/s, accuracy=64.5, test_loss=0.993]\n",
      "Training (epoch 3/5): 100%|██████████████████████████| 781/781 [01:17<00:00, 10.08it/s, batch_acc=68.8, loss_avg=0.799]\n",
      "Testing: 100%|███████████████████████████████████████| 156/156 [00:03<00:00, 48.12it/s, accuracy=66.9, test_loss=0.927]\n",
      "Training (epoch 4/5): 100%|██████████████████████████| 781/781 [00:53<00:00, 14.47it/s, batch_acc=81.2, loss_avg=0.663]\n",
      "Testing: 100%|███████████████████████████████████████| 156/156 [00:04<00:00, 36.91it/s, accuracy=76.1, test_loss=0.688]\n",
      "Training (epoch 5/5): 100%|████████████████████████████| 781/781 [02:28<00:00,  5.25it/s, batch_acc=75, loss_avg=0.573]\n",
      "Testing: 100%|████████████████████████████████████████| 156/156 [00:09<00:00, 16.85it/s, accuracy=73.2, test_loss=0.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WallclockTime: 395.95 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[395.94624042510986]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d5.test_training(executor, train_sampler, validation_set,\n",
    "                 optimizer, epochs, batch_size, output_node,\n",
    "                 metrics=[d5.WallclockTime(reruns=0, avg_over=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data Augmentations into the Mix\n",
    "\n",
    "Instead of observing the same images shuffled, we can create a dataset sampler that augments the input samples by randomly cropping the images, flipping them, and cutting out parts of them. To do so, we reconstruct a new training Sampler, and load reference implementations of data augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep500.frameworks import reference as d5ref\n",
    "train_sampler = d5.ShuffleSampler(train_set, batch_size, transformations=[\n",
    "    d5ref.Crop((32, 32), random_crop=True, padding=(4, 4)),\n",
    "    d5ref.RandomFlip(),\n",
    "    d5ref.Cutout(1, 16),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████| 156/156 [00:04<00:00, 32.84it/s, accuracy=10, test_loss=2.29]\n",
      "Training (epoch 1/5): 100%|████████████████████████████| 781/781 [01:08<00:00, 11.43it/s, batch_acc=48.4, loss_avg=1.7]\n",
      "Testing: 100%|██████████████████████████████████████████| 156/156 [00:03<00:00, 45.57it/s, accuracy=47, test_loss=1.48]\n",
      "Training (epoch 2/5): 100%|████████████████████████████| 781/781 [01:19<00:00,  9.81it/s, batch_acc=64.1, loss_avg=1.3]\n",
      "Testing: 100%|████████████████████████████████████████| 156/156 [00:09<00:00, 15.72it/s, accuracy=55.8, test_loss=1.24]\n",
      "Training (epoch 3/5): 100%|███████████████████████████| 781/781 [03:39<00:00,  3.56it/s, batch_acc=57.8, loss_avg=1.11]\n",
      "Testing: 100%|████████████████████████████████████████| 156/156 [00:10<00:00, 14.97it/s, accuracy=64.5, test_loss=1.07]\n",
      "Training (epoch 4/5): 100%|██████████████████████████| 781/781 [03:18<00:00,  3.93it/s, batch_acc=78.1, loss_avg=0.987]\n",
      "Testing: 100%|███████████████████████████████████████| 156/156 [00:09<00:00, 15.78it/s, accuracy=67.6, test_loss=0.998]\n",
      "Training (epoch 5/5): 100%|██████████████████████████| 781/781 [03:20<00:00,  3.89it/s, batch_acc=65.6, loss_avg=0.899]\n",
      "Testing: 100%|█████████████████████████████████████████| 156/156 [00:10<00:00, 14.84it/s, accuracy=70, test_loss=0.951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WallclockTime: 815.92 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[815.9192576408386]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor = d5fw.from_model(model, device=d5.GPUDevice())\n",
    "optimizer = d5fw.MomentumOptimizer(executor, learning_rate=0.1, momentum=0.9, loss='loss')\n",
    "d5.test_training(executor, train_sampler, validation_set,\n",
    "                 optimizer, epochs, batch_size, output_node,\n",
    "                 metrics=[d5.WallclockTime(reruns=0, avg_over=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seemingly, the results of this run are worse than without data augmentations. However, running the full training procedure for 90 epochs should yield higher overall generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
